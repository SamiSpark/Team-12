{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark \n",
    "from pyspark.sql import HiveContext\n",
    "hc = HiveContext(sc)\n",
    "\n",
    "analys_review_count=hc.sql(\"SELECT categories,A%pyspark\n",
    " \n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.functions import *\n",
    " \n",
    "hcx = HiveContext(sc)\n",
    " \n",
    "df = hcx.table('checkin')\n",
    " \n",
    "result = df.select(col('business_id'), explode(split(col('checkin_dates'), ',')).alias('datetime'))\\\n",
    "    .select('business_id', year(to_timestamp(trim(col('datetime')))).alias('year')) \\\n",
    "    .groupBy('year')\\\n",
    "    .count()\n",
    " \n",
    "z.show(result)VG(review_count) AS avg_review_count FROM business WHERE categories = 'Restaurants, Chinese' OR categories = 'Chinese, Restaurants' GROUP BY categories\")\n",
    "z.show(analys_review_count)\n",
    "\n",
    "%pyspark\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "hcx = HiveContext(sc)\n",
    "\n",
    "df = hcx.table('checkin')\n",
    "\n",
    "result = df.select(col('business_id'), explode(split(col('checkin_dates'), ',')).alias('datetime')) \\\n",
    "    .select('business_id', hour(to_timestamp(trim(col('datetime')))).alias('hour')) \\\n",
    "    .groupBy('hour') \\\n",
    "    .count()\n",
    "\n",
    "z.show(result)\n",
    "\n",
    "%pyspark\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "hcx = HiveContext(sc)\n",
    "\n",
    "checkin_df = hcx.table('checkin')\n",
    "business_df = hcx.table('business')\n",
    "\n",
    "joined_df = checkin_df.join(business_df, checkin_df.business_id == business_df.business_id, 'inner')\n",
    "\n",
    "result = joined_df.groupBy('city').count().orderBy(desc('count'))\n",
    "\n",
    "z.show(result)\n",
    "\n",
    "%pyspark\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.functions import split, size, sum, desc\n",
    "\n",
    "hcx = HiveContext(sc)\n",
    "\n",
    "checkin_df = hcx.table('checkin')\n",
    "business_df = hcx.table('business')\n",
    "\n",
    "# Join the checkin and business tables on business_id\n",
    "joined_df = business_df.join(checkin_df, 'business_id', 'left')\n",
    "\n",
    "# Split the checkin_dates column using comma as the delimiter\n",
    "split_dates = split(joined_df['checkin_dates'], ', ')\n",
    "\n",
    "# Get the count of check-ins by calculating the size of the array\n",
    "joined_df = joined_df.withColumn('checkin_count', size(split_dates))\n",
    "\n",
    "# Group by business_id and name, and calculate the sum of checkin_count\n",
    "result = joined_df.groupBy('business_id', 'name').agg(sum('checkin_count').alias('total_checkin_count')).orderBy(desc('total_checkin_count'))\n",
    "\n",
    "z.show(result)\n",
    "\n",
    "%pyspark\n",
    "from pyspark.sql import HiveContext\n",
    " \n",
    "hc = HiveContext(sc)\n",
    "\n",
    "best_businesses = hc.sql(\"\"\"\n",
    "    SELECT name, state, city, review_count, avg(stars) as avg_stars\n",
    "    FROM business\n",
    "    WHERE review_count > 100\n",
    "    GROUP BY name, state, city, review_count\n",
    "    HAVING avg(stars) = 5\n",
    "    ORDER BY review_count DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "z.show(best_businesses)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
