{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import desc, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Group by city and business ID, and calculate the rating count, average rating, and check-in count\n",
    "grouped_df = joined_df.groupby('city', 'business_id').agg(\n",
    "    count('business_id').alias('rating_count'),\n",
    "    avg('stars').alias('avg_rating'),\n",
    "    count('checkin_dates').alias('checkin_count')\n",
    ")\n",
    "\n",
    "# Rank the businesses within each city based on the three criteria\n",
    "windowSpec = Window.partitionBy('city').orderBy(\n",
    "    desc('rating_count'),\n",
    "    desc('avg_rating'),\n",
    "    desc('checkin_count')\n",
    ")\n",
    "\n",
    "ranked_df = grouped_df.withColumn('rank', rank().over(windowSpec))\n",
    "\n",
    "# Filter the ranked businesses to include only the top 5 for each city\n",
    "top_5_df = ranked_df.filter(ranked_df['rank'] <= 5)\n",
    "\n",
    "# Show the result\n",
    "z.show(top_5_df)\n",
    "\n",
    "%pyspark\n",
    "from pyspark.sql.functions import desc, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Group by city and business ID, and calculate the average rating and review count\n",
    "grouped_df = joined_df.groupby('city', 'business_id').agg(\n",
    "    avg('stars').alias('avg_rating'),\n",
    "    sum('review_count').alias('total_review_count')\n",
    ")\n",
    "\n",
    "# Rank the businesses within each city based on the review count and average rating\n",
    "windowSpec = Window.partitionBy('city').orderBy(\n",
    "    desc('total_review_count'),\n",
    "    desc('avg_rating')\n",
    ")\n",
    "\n",
    "ranked_df = grouped_df.withColumn('rank', rank().over(windowSpec))\n",
    "\n",
    "# Filter the ranked businesses to include only the top 5 for each city\n",
    "top_5_df = ranked_df.filter(ranked_df['rank'] <= 5)\n",
    "\n",
    "# Show the result\n",
    "z.show(top_5_df)\n",
    "\n",
    "\n",
    "%pyspark\n",
    "from pyspark.sql.functions import desc, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Group by city, business ID, and name, and calculate the average rating and review count\n",
    "grouped_df = joined_df.groupby('city', 'business_id', 'name').agg(\n",
    "    avg('stars').alias('avg_rating'),\n",
    "    sum('review_count').alias('total_review_count')\n",
    ")\n",
    "\n",
    "# Rank the businesses within each city based on the review count and average rating\n",
    "windowSpec = Window.partitionBy('city').orderBy(\n",
    "    desc('total_review_count'),\n",
    "    desc('avg_rating')\n",
    ")\n",
    "\n",
    "ranked_df = grouped_df.withColumn('rank', rank().over(windowSpec))\n",
    "\n",
    "# Filter the ranked businesses to include only the top 5 for each city\n",
    "top_5_df = ranked_df.filter(ranked_df['rank'] <= 5)\n",
    "\n",
    "# Show the result\n",
    "z.show(top_5_df,Truncate=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
